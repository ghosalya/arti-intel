{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Q1**] _The following is the AdaGrad algorithm for weight update._\n",
    "\n",
    "$$ cache_i = cache_i + (∇_{w_i}L)^2 $$\n",
    "\n",
    "$$ w_i = w_i - \\frac{η}{\\sqrt{cache_i} + \\epsilon}∇_{w_i}L $$\n",
    "\n",
    "_where $w_i$ is the weight to be updated, $∇_{w_i}L$ is the gradient of the loss w.r.t $w_i$ , $\\epsilon$ is a hyperparemeter between $10^{−8}$ and $10^{−4}$ and η is a hyperparameter similar to step size in SGD. List one difference between AdaGrad and SGD in terms of step size and **explain** what effects you expect from this difference._\n",
    "\n",
    "**ANSWER: **\n",
    "\n",
    "SGD use constant step size, which suffers from descending too slow in a flat, non-minumum areas and descending too fast in the steeper area. \n",
    "\n",
    "$$ w_i = w_i - η∇_{w_i}L $$\n",
    "\n",
    "AdaGrad attempts to mitigate this by introducing a _cache_; in this case it is the sum of the previous gradients squared. By dividing the gradient descent by the square root of the cache, this means that the gradient will descend faster when the cache is small, and slower when the cache is big.\n",
    "\n",
    "Small cache means that the gradient has been small for the past iterations, meaning that the model is currently at a \"flat\" area of the graph, and it should move faster as everything around the area is generally going to be flat, so that the model can move to an area that actually has a minimum. Big cache means that the gradient has been big for the past iterations, implying a \"steep\" area in which the model should descend more slowly on, in order to not miss the minimum point. To this end, AdaGrad will perform better than SGD in terms of both speed and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Q2**] The following are the defining equations for an LSTM cell,\n",
    "\n",
    "$$ i_t = \\sigma(W^i_{x_t} + U^i h_{t-1}) $$\n",
    "$$ f_t = \\sigma(W^f_{x_t} + U^f h_{t-1}) $$\n",
    "$$ o_t = \\sigma(W^o_{x_t} + U^o h_{t-1}) $$\n",
    "$$ \\hat{c_t} = tanh(W^c_{x_t} + U^c h_{t-1}) $$\n",
    "$$ c = f_t ◦ c_{t-1} + i_t ◦ \\hat{c_t} $$\n",
    "$$ h_t = o_t ◦ tanh(c_t) $$\n",
    "\n",
    "The symbol $◦$ denotes element-wise multiplication and $\\sigma(x) = \\frac{1}{1+e^{−x}}$ is the sigmoid function. Answer True/False to the following questions and give not more than 2 sentences explanation. \n",
    "1. If $x_t = 0$ vector then $h_t = h_{t−1}$. \n",
    "2. If $f_t$ is very small or zero, then the error will not be back-propagated to earlier time steps.\n",
    "3. The entries of $f_t,i_t,o_t$ are non-negative.\n",
    "4. $f_t,i_t,o_t$ can be seen as probability distributions, which means that their entries are nonnegative and their entries sum to 1.\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "1. False. The cell forward pass is still affected by $U^* \\cdot h_{t-1}$ (where $* \\epsilon \\{i, f, o\\}$). (??)\n",
    "2. ?\n",
    "3. True. As $i_t, f_t, 0_t$ is the output of sigmoid functions, it will only lie in (0, 1) as output. \n",
    "4. False. While the entries are non-negative, they graph does not sum up to 1. As $\\sigma(\\infty) = 1$, it is impossible for the sum of the graph to be equal to 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Q3**]  The deﬁning equations for a GRU cell are, \n",
    "\n",
    "$$ z_t = \\sigma(W^z x_t + u^z h_{t-1}) $$\n",
    "$$ r_t = \\sigma(W^r x_t + u^r h_{t-1}) $$\n",
    "$$ \\hat h_t = tanh(Wx_t + r_t ◦ Uh_{t-1}) $$\n",
    "$$ h_t = z_t ◦ h_{t-1} + (1 - z_t) ◦ \\hat h_t $$\n",
    "\n",
    "1. Draw a diagram of this GRU cell.\n",
    "2. Assume $h_t$ and $x_t$ are column vectors, with dimensions $d_h$ and $d_x$ respectively. What are the dimensions (rows × columns) of the weight matrices $W^z,W^r,W,U^z,U^r,$ and $U$? \n",
    "3. Like LSTM cells, GRU cells can tackle vanishing or exploding gradient problem too. By taking a look at the formula for LSTM in Q2, what is the main advantage of using GRU cells over LSTMs for some problems? Give an answer it at most 5 sentences. _Hint: We expect a qualitative answer (deep math proofs are not required) that comes with an explanation of the answer._\n",
    "\n",
    "**ANSWERS**\n",
    "\n",
    "1. GRU Cell diagram is as follows (drawn with [draw.io](http://www.draw.io))\n",
    "![gru-cell](grucell.jpg)\n",
    "2. Corresponding dimensions:\n",
    "$$ from \\space their \\space matrix \\space multiplications: $$\n",
    "$$ d_{W}^z = a × d_x, \\quad d_{U}^z = a × d_h, \\quad d_{z} = a × 1$$\n",
    "$$ d_{W}^r = b × d_x, \\quad d_{U}^r = b × d_h, \\quad d_{r} = b × 1$$\n",
    "$$ d_{U} = c × d_h $$\n",
    "$$ $$\n",
    "$$ from \\space their \\space elementwise \\space multiplications: $$\n",
    "$$ d_{z} = d_h × 1 , \\quad a = d_h$$\n",
    "$$ d_{r} = d_h × 1 , \\quad b = d_h$$\n",
    "$$ c = d_h $$\n",
    "$$ $$\n",
    "$$ therefore: $$\n",
    "$$ d_{W}^z = d_h × d_x, \\quad d_{U}^z = d_h × d_h$$\n",
    "$$ d_{W}^r = d_h × d_x, \\quad d_{U}^r = d_h × d_h$$\n",
    "$$ d_{U} = d_h × d_h $$\n",
    "$$ $$\n",
    "3. The main advantage is that GRU has 2 gates instead of 3, meaning that the parameters required to train in GRU is less than that of LSTM; assuming they perform with the same accuracy, GRU is more computationally efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KIRK This ship is in trouble. We'd better start solving problems faster than we pick up new ones. We seem to be in the middle of a creeping paralysis. Mister Spock, analysis of that last burst of noise before we started losing power.\n",
      "KIRK Zefram Cochrane of Alpha Centuri, the discoverer of the space warp\n",
      "MCCOY In the pink, as usual. Liver, kidney, blood count, metabolic rate. Everything, even your glands, functioning at their usual, normal peak of efficiency.\n",
      "MCCOY In the midst of what seems so unreal, the harsh reality. This is not a dream.\n",
      "SPOCK It is not logical that you are Surak. There is no fact, extrapolation of fact or theory, which would make possible.\n"
     ]
    }
   ],
   "source": [
    "from hw6_code import *\n",
    "\n",
    "star_filter = ['NEXTEPISODE']\n",
    "dataset = MovieScriptDataset('../dataset/startrek/star_trek_transcripts_all_episodes_f.csv',\n",
    "                             filterwords=star_filter)\n",
    "train_data, test_data = dataset.split_train_test()\n",
    "print(train_data.line_list[-1], '\\n')\n",
    "print(train_data.line_list[-3], '\\n')\n",
    "print(train_data.line_list[-5], '\\n')\n",
    "print(train_data.line_list[-10], '\\n')\n",
    "print(train_data.line_list[-50], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_mod = CoveredLSTM(len(charspace), 200, 3, len(charspace)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-2:\n",
      "Process Process-4:\n",
      "Process Process-1:\n",
      "Process Process-3:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-12d88cae282f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_mod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/arti-intel/week6/hw6_code.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, model, batch_size, use_gpu, mode, lr, epoch, print_every, sample_every)\u001b[0m\n\u001b[1;32m    277\u001b[0m                                  batch_label.long())\n\u001b[1;32m    278\u001b[0m                 \u001b[0;31m# print('itme', loss.item())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m                 \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbatch_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                 \u001b[0mtotal_letters\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_in_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"training\")\n",
    "trained_model, loss, acc = train(train_data, lstm_mod, lr=5e-2, batch_size=32, mode='train', epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"On testing data\")\n",
    "final_model, loss, acc = train(test_data, trained_model, lr=5e-2, batch_size=8, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling 50 sentences\n",
    "for i in range(50):\n",
    "    print('GENERATED:', final_model.sample())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
