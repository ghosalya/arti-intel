{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Q1**] _The following is the AdaGrad algorithm for weight update._\n",
    "\n",
    "$$ cache_i = cache_i + (∇_{w_i}L)^2 $$\n",
    "\n",
    "$$ w_i = w_i - \\frac{η}{\\sqrt{cache_i} + \\epsilon}∇_{w_i}L $$\n",
    "\n",
    "_where $w_i$ is the weight to be updated, $∇_{w_i}L$ is the gradient of the loss w.r.t $w_i$ , $\\epsilon$ is a hyperparemeter between $10^{−8}$ and $10^{−4}$ and η is a hyperparameter similar to step size in SGD. List one difference between AdaGrad and SGD in terms of step size and **explain** what effects you expect from this difference._\n",
    "\n",
    "**ANSWER: **\n",
    "\n",
    "SGD use constant step size, which suffers from descending too slow in a flat, non-minumum areas and descending too fast in the steeper area. \n",
    "\n",
    "$$ w_i = w_i - η∇_{w_i}L $$\n",
    "\n",
    "AdaGrad attempts to mitigate this by introducing a _cache_; in this case it is the sum of the previous gradients squared. By dividing the gradient descent by the square root of the cache, this means that the gradient will descend faster when the cache is small, and slower when the cache is big.\n",
    "\n",
    "Small cache means that the gradient has been small for the past iterations, meaning that the model is currently at a \"flat\" area of the graph, and it should move faster as everything around the area is generally going to be flat, so that the model can move to an area that actually has a minimum. Big cache means that the gradient has been big for the past iterations, implying a \"steep\" area in which the model should descend more slowly on, in order to not miss the minimum point. To this end, AdaGrad will perform better than SGD in terms of both speed and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Q2**] The following are the defining equations for an LSTM cell,\n",
    "\n",
    "$$ i_t = \\sigma(W^i_{x_t} + U^i h_{t-1}) $$\n",
    "$$ f_t = \\sigma(W^f_{x_t} + U^f h_{t-1}) $$\n",
    "$$ o_t = \\sigma(W^o_{x_t} + U^o h_{t-1}) $$\n",
    "$$ \\hat{c_t} = tanh(W^c_{x_t} + U^c h_{t-1}) $$\n",
    "$$ c = f_t ◦ c_{t-1} + i_t ◦ \\hat{c_t} $$\n",
    "$$ h_t = o_t ◦ tanh(c_t) $$\n",
    "\n",
    "The symbol $◦$ denotes element-wise multiplication and $\\sigma(x) = \\frac{1}{1+e^{−x}}$ is the sigmoid function. Answer True/False to the following questions and give not more than 2 sentences explanation. \n",
    "1. If $x_t = 0$ vector then $h_t = h_{t−1}$. \n",
    "2. If $f_t$ is very small or zero, then the error will not be back-propagated to earlier time steps.\n",
    "3. The entries of $f_t,i_t,o_t$ are non-negative.\n",
    "4. $f_t,i_t,o_t$ can be seen as probability distributions, which means that their entries are nonnegative and their entries sum to 1.\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "1. False. The cell forward pass is still affected by $U^* \\cdot h_{t-1}$ (where $* \\epsilon \\{i, f, o\\}$).\n",
    "2. False. Backpropagation will still occur according to $i_t$ and $o_t$.\n",
    "3. True. As $i_t, f_t, o_t$ is the output of sigmoid functions, it will only lie in (0, 1) as output. \n",
    "4. False. While the entries are non-negative, they graph does not sum up to 1. As $\\sigma(\\infty) = 1$, it is impossible for the sum of the graph to be equal to 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Q3**]  The deﬁning equations for a GRU cell are, \n",
    "\n",
    "$$ z_t = \\sigma(W^z x_t + u^z h_{t-1}) $$\n",
    "$$ r_t = \\sigma(W^r x_t + u^r h_{t-1}) $$\n",
    "$$ \\hat h_t = tanh(Wx_t + r_t ◦ Uh_{t-1}) $$\n",
    "$$ h_t = z_t ◦ h_{t-1} + (1 - z_t) ◦ \\hat h_t $$\n",
    "\n",
    "1. Draw a diagram of this GRU cell.\n",
    "2. Assume $h_t$ and $x_t$ are column vectors, with dimensions $d_h$ and $d_x$ respectively. What are the dimensions (rows × columns) of the weight matrices $W^z,W^r,W,U^z,U^r,$ and $U$? \n",
    "3. Like LSTM cells, GRU cells can tackle vanishing or exploding gradient problem too. By taking a look at the formula for LSTM in Q2, what is the main advantage of using GRU cells over LSTMs for some problems? Give an answer it at most 5 sentences. _Hint: We expect a qualitative answer (deep math proofs are not required) that comes with an explanation of the answer._\n",
    "\n",
    "**ANSWERS**\n",
    "\n",
    "1. GRU Cell diagram is as follows (drawn with [draw.io](http://www.draw.io))\n",
    "![gru-cell](grucell.jpg)\n",
    "2. Corresponding dimensions:\n",
    "$$ from \\space their \\space matrix \\space multiplications: $$\n",
    "$$ d_{W}^z = a × d_x, \\quad d_{U}^z = a × d_h, \\quad d_{z} = a × 1$$\n",
    "$$ d_{W}^r = b × d_x, \\quad d_{U}^r = b × d_h, \\quad d_{r} = b × 1$$\n",
    "$$ d_{U} = c × d_h $$\n",
    "$$ $$\n",
    "$$ from \\space their \\space elementwise \\space multiplications: $$\n",
    "$$ d_{z} = d_h × 1 , \\quad a = d_h$$\n",
    "$$ d_{r} = d_h × 1 , \\quad b = d_h$$\n",
    "$$ c = d_h $$\n",
    "$$ $$\n",
    "$$ therefore: $$\n",
    "$$ d_{W}^z = d_h × d_x, \\quad d_{U}^z = d_h × d_h$$\n",
    "$$ d_{W}^r = d_h × d_x, \\quad d_{U}^r = d_h × d_h$$\n",
    "$$ d_{U} = d_h × d_h $$\n",
    "$$ $$\n",
    "3. The main advantage is that GRU has 2 gates instead of 3, meaning that the parameters required to train in GRU is less than that of LSTM; assuming they perform with the same accuracy, GRU is more computationally efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KIRK Beautifully done. \n",
      "\n",
      "SPOCK But the computer is inaccurate, never the less. \n",
      "\n",
      "KIRK So we were told. Mister Spock, the pictures. The children are waiting. \n",
      "\n",
      "SEVEN  Yes, I heard him, Isis. We're aboard a space vessel. From what planet \n",
      "\n",
      "SPOCK Quadrotriticale is a highyield grain, a fourlobed hybrid of wheat and rye. A perennial, also, I believe. Its root grain, triticale, can trace its ancestry all the way back to twentieth century Canada \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from hw6_code import *\n",
    "\n",
    "star_filter = ['NEXTEPISODE']\n",
    "dataset = MovieScriptDataset('../dataset/startrek/star_trek_transcripts_all_episodes_f.csv',\n",
    "                             filterwords=star_filter)\n",
    "train_data, test_data = dataset.split_train_test()\n",
    "print(train_data.line_list[-1], '\\n')\n",
    "print(train_data.line_list[-3], '\\n')\n",
    "print(train_data.line_list[-5], '\\n')\n",
    "print(train_data.line_list[-10], '\\n')\n",
    "print(train_data.line_list[-50], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_mod = CoveredLSTM(len(charspace), 200, 3, len(charspace)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "EPOCH 0\n",
      "output torch.Size([59])/2881\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 41], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 30], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 28], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 39], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 41], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 28], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 28], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 41], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 41], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 41], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 41], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 26], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 41], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 40], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 41], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 28], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 26], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 28], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 41], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 41], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 30], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 41], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 28], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 28], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 28], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 40], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 41], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 43], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 40], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 40], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 28], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 41], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 28], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 46], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 40], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 28], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 30], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 41], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "output torch.Size([59])\n",
      "mult tensor([ 34], device='cuda:0')\n",
      "      generated_sample: CIIIIPIIIIIIECIINIPIICIICIIIIIPIPPIIIIIPIAIPOIIIIIPIIICAIIIICPIPEPICCCIOPRIOIIOIICPIIIICIIUIOIICIEPII [loss:2909.07853 || acc:0.210 || in 149.0173s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "backward_input can only be called in training mode",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-fde21ece859c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"training\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m trained_model, train_loss_acc, test_loss_acc = train(train_data, test_data, lstm_mod, resume_from=0,\n\u001b[1;32m----> 3\u001b[1;33m                                  learnrate=5e-1, batch_size=8, epoch=5, sample_every=1000, save_model_every=50)\n\u001b[0m",
      "\u001b[1;32m~\\arti-intel\\week6\\hw6_code.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_dataset, test_dataset, model, batch_size, use_gpu, learnrate, epoch, print_every, sample_every, resume_from, save_model_every)\u001b[0m\n\u001b[0;32m    262\u001b[0m         model, loss, acc = train_single(train_dataset, model, optimizer, criterion,\n\u001b[0;32m    263\u001b[0m                                         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m                                         print_every=print_every, sample_every=sample_every)\n\u001b[0m\u001b[0;32m    265\u001b[0m         \u001b[0mtrain_loss_acc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[1;31m# testing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\arti-intel\\week6\\hw6_code.py\u001b[0m in \u001b[0;36mtrain_single\u001b[1;34m(dataset, model, optimizer, criterion, batch_size, use_gpu, mode, print_every, sample_every)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: backward_input can only be called in training mode"
     ]
    }
   ],
   "source": [
    "print(\"training\")\n",
    "trained_model, train_loss_acc, test_loss_acc = train(train_data, test_data, lstm_mod, resume_from=0,\n",
    "                                 learnrate=2.5e-1, batch_size=8, epoch=5, sample_every=1000, save_model_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_over_epoch(train_loss_acc, test_loss_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling 50 sentences\n",
    "for i in range(50):\n",
    "    print('GENERATED:', final_model.sample())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
