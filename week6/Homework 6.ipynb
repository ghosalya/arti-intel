{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Q1**] _The following is the AdaGrad algorithm for weight update._\n",
    "\n",
    "$$ cache_i = cache_i + (∇_{w_i}L)^2 $$\n",
    "\n",
    "$$ w_i = w_i - \\frac{η}{\\sqrt{cache_i} + \\epsilon}∇_{w_i}L $$\n",
    "\n",
    "_where $w_i$ is the weight to be updated, $∇_{w_i}L$ is the gradient of the loss w.r.t $w_i$ , $\\epsilon$ is a hyperparemeter between $10^{−8}$ and $10^{−4}$ and η is a hyperparameter similar to step size in SGD. List one difference between AdaGrad and SGD in terms of step size and **explain** what effects you expect from this difference._\n",
    "\n",
    "**ANSWER: **\n",
    "\n",
    "SGD use constant step size, which suffers from descending too slow in a flat, non-minumum areas and descending too fast in the steeper area. \n",
    "\n",
    "$$ w_i = w_i - η∇_{w_i}L $$\n",
    "\n",
    "AdaGrad attempts to mitigate this by introducing a _cache_; in this case it is the sum of the previous gradients squared. By dividing the gradient descent by the square root of the cache, this means that the gradient will descend faster when the cache is small, and slower when the cache is big.\n",
    "\n",
    "Small cache means that the gradient has been small for the past iterations, meaning that the model is currently at a \"flat\" area of the graph, and it should move faster as everything around the area is generally going to be flat, so that the model can move to an area that actually has a minimum. Big cache means that the gradient has been big for the past iterations, implying a \"steep\" area in which the model should descend more slowly on, in order to not miss the minimum point. To this end, AdaGrad will perform better than SGD in terms of both speed and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Q2**] The following are the defining equations for an LSTM cell,\n",
    "\n",
    "$$ i_t = \\sigma(W^i_{x_t} + U^i h_{t-1}) $$\n",
    "$$ f_t = \\sigma(W^f_{x_t} + U^f h_{t-1}) $$\n",
    "$$ o_t = \\sigma(W^o_{x_t} + U^o h_{t-1}) $$\n",
    "$$ \\hat{c_t} = tanh(W^c_{x_t} + U^c h_{t-1}) $$\n",
    "$$ c = f_t ◦ c_{t-1} + i_t ◦ \\hat{c_t} $$\n",
    "$$ h_t = o_t ◦ tanh(c_t) $$\n",
    "\n",
    "The symbol $◦$ denotes element-wise multiplication and $\\sigma(x) = \\frac{1}{1+e^{−x}}$ is the sigmoid function. Answer True/False to the following questions and give not more than 2 sentences explanation. \n",
    "1. If $x_t = 0$ vector then $h_t = h_{t−1}$. \n",
    "2. If $f_t$ is very small or zero, then the error will not be back-propagated to earlier time steps.\n",
    "3. The entries of $f_t,i_t,o_t$ are non-negative.\n",
    "4. $f_t,i_t,o_t$ can be seen as probability distributions, which means that their entries are nonnegative and their entries sum to 1.\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "1. False. The cell forward pass is still affected by $U^* \\cdot h_{t-1}$ (where $* \\epsilon \\{i, f, o\\}$).\n",
    "2. False. Backpropagation will still occur according to $i_t$ and $o_t$.\n",
    "3. True. As $i_t, f_t, o_t$ is the output of sigmoid functions, it will only lie in (0, 1) as output. \n",
    "4. False. While the entries are non-negative, they graph does not sum up to 1. As $\\sigma(\\infty) = 1$, it is impossible for the sum of the graph to be equal to 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Q3**]  The deﬁning equations for a GRU cell are, \n",
    "\n",
    "$$ z_t = \\sigma(W^z x_t + u^z h_{t-1}) $$\n",
    "$$ r_t = \\sigma(W^r x_t + u^r h_{t-1}) $$\n",
    "$$ \\hat h_t = tanh(Wx_t + r_t ◦ Uh_{t-1}) $$\n",
    "$$ h_t = z_t ◦ h_{t-1} + (1 - z_t) ◦ \\hat h_t $$\n",
    "\n",
    "1. Draw a diagram of this GRU cell.\n",
    "2. Assume $h_t$ and $x_t$ are column vectors, with dimensions $d_h$ and $d_x$ respectively. What are the dimensions (rows × columns) of the weight matrices $W^z,W^r,W,U^z,U^r,$ and $U$? \n",
    "3. Like LSTM cells, GRU cells can tackle vanishing or exploding gradient problem too. By taking a look at the formula for LSTM in Q2, what is the main advantage of using GRU cells over LSTMs for some problems? Give an answer it at most 5 sentences. _Hint: We expect a qualitative answer (deep math proofs are not required) that comes with an explanation of the answer._\n",
    "\n",
    "**ANSWERS**\n",
    "\n",
    "1. GRU Cell diagram is as follows (drawn with [draw.io](http://www.draw.io))\n",
    "![gru-cell](grucell.jpg)\n",
    "2. Corresponding dimensions:\n",
    "$$ from \\space their \\space matrix \\space multiplications: $$\n",
    "$$ d_{W}^z = a × d_x, \\quad d_{U}^z = a × d_h, \\quad d_{z} = a × 1$$\n",
    "$$ d_{W}^r = b × d_x, \\quad d_{U}^r = b × d_h, \\quad d_{r} = b × 1$$\n",
    "$$ d_{U} = c × d_h $$\n",
    "$$ $$\n",
    "$$ from \\space their \\space elementwise \\space multiplications: $$\n",
    "$$ d_{z} = d_h × 1 , \\quad a = d_h$$\n",
    "$$ d_{r} = d_h × 1 , \\quad b = d_h$$\n",
    "$$ c = d_h $$\n",
    "$$ $$\n",
    "$$ therefore: $$\n",
    "$$ d_{W}^z = d_h × d_x, \\quad d_{U}^z = d_h × d_h$$\n",
    "$$ d_{W}^r = d_h × d_x, \\quad d_{U}^r = d_h × d_h$$\n",
    "$$ d_{U} = d_h × d_h $$\n",
    "$$ $$\n",
    "3. The main advantage is that GRU has 2 gates instead of 3, meaning that the parameters required to train in GRU is less than that of LSTM; assuming they perform with the same accuracy, GRU is more computationally efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KIRK : How's our number one passenger doing? \n",
      "\n",
      "KIRK: Of course. Perhaps I'll see \n",
      "\n",
      "SPOCK: It is in no known code. There's no detectable pattern. No standard references apply. \n",
      "\n",
      "SCOTT : Queen to queen's level three. Repeat. Queen to queen's level three. \n",
      "\n",
      "TRELANE: Transporter signal? What does he mean? You must tell me! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from hw6_code import *\n",
    "\n",
    "star_filter = ['NEXTEPISODE']\n",
    "dataset = MovieScriptDataset('../dataset/startrek/star_trek_transcripts_all_episodes_f.csv',\n",
    "                             filterwords=star_filter)\n",
    "train_data, test_data = dataset.split_train_test()\n",
    "print(train_data.line_list[-1], '\\n')\n",
    "print(train_data.line_list[-3], '\\n')\n",
    "print(train_data.line_list[-5], '\\n')\n",
    "print(train_data.line_list[-10], '\\n')\n",
    "print(train_data.line_list[-50], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_mod = CoveredLSTM(len(charspace), 200, 3, len(charspace)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "EPOCH 0\n",
      "      >> Epoch loss 4.10607 accuracy 0.177                    in 182.5223s\n",
      "      >> Epoch loss 1.70931 accuracy 0.188                    in 40.2731s\n",
      "      sample line: SCOTT: Aye, sir.\n",
      "      sample output: SIRKK:          \n",
      "EPOCH 1\n",
      "      >> Epoch loss 3.44839 accuracy 0.268                    in 181.4306s\n",
      "      >> Epoch loss 1.24799 accuracy 0.351                    in 41.6346s\n",
      "      sample line: ALICES: You desire something, lord?\n",
      "      sample output: AICR::: Iou touene tore  eng  tor  \n",
      "EPOCH 2\n",
      "      ...iteration 517/1441\r"
     ]
    }
   ],
   "source": [
    "print(\"training\")\n",
    "trained_model, train_loss_acc, test_loss_acc = train(train_data, test_data, lstm_mod, resume_from=0, save_model_every=5,\n",
    "                                                     learnrate=1e-1, batch_size=16, sample_every=5000, epoch=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_over_epoch(train_loss_acc, test_loss_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling 50 sentences\n",
    "for i in range(50):\n",
    "    print('GENERATED:', trained_model.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
