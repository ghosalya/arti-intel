{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Q1**] _The following is the AdaGrad algorithm for weight update._\n",
    "\n",
    "$$ cache_i = cache_i + (∇_{w_i}L)^2 $$\n",
    "\n",
    "$$ w_i = w_i - \\frac{η}{\\sqrt{cache_i} + \\epsilon}∇_{w_i}L $$\n",
    "\n",
    "_where $w_i$ is the weight to be updated, $∇_{w_i}L$ is the gradient of the loss w.r.t $w_i$ , $\\epsilon$ is a hyperparemeter between $10^{−8}$ and $10^{−4}$ and η is a hyperparameter similar to step size in SGD. List one difference between AdaGrad and SGD in terms of step size and **explain** what effects you expect from this difference._\n",
    "\n",
    "**ANSWER: **\n",
    "\n",
    "SGD use constant step size, which suffers from descending too slow in a flat, non-minumum areas and descending too fast in the steeper area. \n",
    "\n",
    "$$ w_i = w_i - η∇_{w_i}L $$\n",
    "\n",
    "AdaGrad attempts to mitigate this by introducing a _cache_; in this case it is the sum of the previous gradients squared. By dividing the gradient descent by the square root of the cache, this means that the gradient will descend faster when the cache is small, and slower when the cache is big.\n",
    "\n",
    "Small cache means that the gradient has been small for the past iterations, meaning that the model is currently at a \"flat\" area of the graph, and it should move faster as everything around the area is generally going to be flat, so that the model can move to an area that actually has a minimum. Big cache means that the gradient has been big for the past iterations, implying a \"steep\" area in which the model should descend more slowly on, in order to not miss the minimum point. To this end, AdaGrad will perform better than SGD in terms of both speed and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Q2**] The following are the defining equations for an LSTM cell,\n",
    "\n",
    "$$ i_t = \\sigma(W^i_{x_t} + U^i h_{t-1}) $$\n",
    "$$ f_t = \\sigma(W^f_{x_t} + U^f h_{t-1}) $$\n",
    "$$ o_t = \\sigma(W^o_{x_t} + U^o h_{t-1}) $$\n",
    "$$ \\hat{c_t} = tanh(W^c_{x_t} + U^c h_{t-1}) $$\n",
    "$$ c = f_t ◦ c_{t-1} + i_t ◦ \\hat{c_t} $$\n",
    "$$ h_t = o_t ◦ tanh(c_t) $$\n",
    "\n",
    "The symbol $◦$ denotes element-wise multiplication and $\\sigma(x) = \\frac{1}{1+e^{−x}}$ is the sigmoid function. Answer True/False to the following questions and give not more than 2 sentences explanation. \n",
    "1. If $x_t = 0$ vector then $h_t = h_{t−1}$. \n",
    "2. If $f_t$ is very small or zero, then the error will not be back-propagated to earlier time steps.\n",
    "3. The entries of $f_t,i_t,o_t$ are non-negative.\n",
    "4. $f_t,i_t,o_t$ can be seen as probability distributions, which means that their entries are nonnegative and their entries sum to 1.\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "1. False. The cell forward pass is still affected by $U^* \\cdot h_{t-1}$ (where $* \\epsilon \\{i, f, o\\}$). (??)\n",
    "2. ?\n",
    "3. True. As $i_t, f_t, 0_t$ is the output of sigmoid functions, it will only lie in (0, 1) as output. \n",
    "4. False. While the entries are non-negative, they graph does not sum up to 1. As $\\sigma(\\infty) = 1$, it is impossible for the sum of the graph to be equal to 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Q3**]  The deﬁning equations for a GRU cell are, \n",
    "\n",
    "$$ z_t = \\sigma(W^z x_t + u^z h_{t-1}) $$\n",
    "$$ r_t = \\sigma(W^r x_t + u^r h_{t-1}) $$\n",
    "$$ \\hat h_t = tanh(Wx_t + r_t ◦ Uh_{t-1}) $$\n",
    "$$ h_t = z_t ◦ h_{t-1} + (1 - z_t) ◦ \\hat h_t $$\n",
    "\n",
    "1. Draw a diagram of this GRU cell.\n",
    "2. Assume $h_t$ and $x_t$ are column vectors, with dimensions $d_h$ and $d_x$ respectively. What are the dimensions (rows × columns) of the weight matrices $W^z,W^r,W,U^z,U^r,$ and $U$? \n",
    "3. Like LSTM cells, GRU cells can tackle vanishing or exploding gradient problem too. By taking a look at the formula for LSTM in Q2, what is the main advantage of using GRU cells over LSTMs for some problems? Give an answer it at most 5 sentences. _Hint: We expect a qualitative answer (deep math proofs are not required) that comes with an explanation of the answer._\n",
    "\n",
    "**ANSWERS**\n",
    "\n",
    "1. GRU Cell diagram is as follows (drawn with [draw.io](http://www.draw.io))\n",
    "![gru-cell](grucell.jpg)\n",
    "2. Corresponding dimensions:\n",
    "$$ from \\space their \\space matrix \\space multiplications: $$\n",
    "$$ d_{W}^z = a × d_x, \\quad d_{U}^z = a × d_h, \\quad d_{z} = a × 1$$\n",
    "$$ d_{W}^r = b × d_x, \\quad d_{U}^r = b × d_h, \\quad d_{r} = b × 1$$\n",
    "$$ d_{U} = c × d_h $$\n",
    "$$ $$\n",
    "$$ from \\space their \\space elementwise \\space multiplications: $$\n",
    "$$ d_{z} = d_h × 1 , \\quad a = d_h$$\n",
    "$$ d_{r} = d_h × 1 , \\quad b = d_h$$\n",
    "$$ c = d_h $$\n",
    "$$ $$\n",
    "$$ therefore: $$\n",
    "$$ d_{W}^z = d_h × d_x, \\quad d_{U}^z = d_h × d_h$$\n",
    "$$ d_{W}^r = d_h × d_x, \\quad d_{U}^r = d_h × d_h$$\n",
    "$$ d_{U} = d_h × d_h $$\n",
    "$$ $$\n",
    "3. The main advantage is that GRU has 2 gates instead of 3, meaning that the parameters required to train in GRU is less than that of LSTM; assuming they perform with the same accuracy, GRU is more computationally efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCCOY Don't move a muscle. \n",
      "\n",
      "TAMAR Surely, Hacom, they already know. Are they not infallible \n",
      "\n",
      "KIRK Mister Chekov, you be careful. Mister Spock, except for our hand guns, we haven't changed, not even our clothing, yet these people see and hear us as the Clantons. \n",
      "\n",
      "KIRK Now. \n",
      "\n",
      "HENGIST Oh, no. Argelius hires its administrative officers from other planets. The Argelians aren't very efficient, you know. Gentle, harmless people. Mister Scott. Mister Scott Now, are you sure you've never seen this before \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from hw6_code import *\n",
    "\n",
    "star_filter = ['NEXTEPISODE']\n",
    "dataset = MovieScriptDataset('../dataset/startrek/star_trek_transcripts_all_episodes_f.csv',\n",
    "                             filterwords=star_filter)\n",
    "train_data, test_data = dataset.split_train_test()\n",
    "print(train_data.line_list[-1], '\\n')\n",
    "print(train_data.line_list[-3], '\\n')\n",
    "print(train_data.line_list[-5], '\\n')\n",
    "print(train_data.line_list[-10], '\\n')\n",
    "print(train_data.line_list[-50], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_mod = CoveredLSTM(len(charspace), 200, 3, len(charspace)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "      ...iteration 2/721\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-4:\n",
      "Process Process-2:\n",
      "Process Process-3:\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f2b9606aac8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 347, in __del__\n",
      "    def __del__(self):\n",
      "  File \"/home/ubuntu/anaconda3/envs/ai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 178, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 1918) exited unexpectedly with exit code 1.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-12d88cae282f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_mod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/arti-intel/week6/hw6_code.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, model, batch_size, use_gpu, mode, lr, epoch, print_every, sample_every)\u001b[0m\n\u001b[1;32m    285\u001b[0m                     \u001b[0;31m# retain_graph must be set to True except the last one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                     \u001b[0;31m# which is when all batches has been processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"training\")\n",
    "trained_model, loss, acc = train(train_data, lstm_mod, lr=5e-2, batch_size=32, mode='train', epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"On testing data\")\n",
    "final_model, loss, acc = train(test_data, trained_model, lr=5e-2, batch_size=8, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling 50 sentences\n",
    "for i in range(50):\n",
    "    print('GENERATED:', final_model.sample())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
